{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Keras-NLP"
      ],
      "metadata": {
        "id": "VxF9Nw5IRRly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Needed Dependencies"
      ],
      "metadata": {
        "id": "aU3HfxXeRUHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFkvAY7NOSHY",
        "outputId": "19bd7803-5a09-47d2-8eac-7ba17eabd76a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "# Use mixed precision to speed up all training in this guide.\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "jVxvHjojOZQg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with a pretrained classifier"
      ],
      "metadata": {
        "id": "AEdifiyxLh8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qttlq6dPuoLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004dba9c-c9c7-4459-f235-2e3fd8f7e50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/3/download/config.json...\n",
            "100%|██████████| 2.14k/2.14k [00:00<00:00, 316kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/3/download/assets/tokenizer/vocabulary.txt...\n",
            "100%|██████████| 226k/226k [00:00<00:00, 6.52MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/3/download/model.weights.h5...\n",
            "100%|██████████| 16.8M/16.8M [00:00<00:00, 68.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 2 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.539,  1.542]], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "# Note: batched inputs expected so must wrap string in iterable\n",
        "classifier.predict([\"I love modular workflows in keras-nlp!\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!# Remove unsupervised examples\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD3S5ohERNuu",
        "outputId": "98010a32-2b8b-47e9-f05d-0c43383e2cab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  7398k      0  0:00:11  0:00:11 --:--:-- 10.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "imdb_train = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "imdb_test = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Inspect first review\n",
        "# Format is (review text tensor, label tensor)\n",
        "print(imdb_train.unbatch().take(1).get_single_element())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6UIIkwlRNbA",
        "outputId": "f46c7ace-2829-4f9c-ba1c-b52f5fd17ae8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"I watched the pilot and noticed more than a few similarities between 3 lbs and House, M.D.. Tucci's character is brilliant but socially inept out of choice, similar to Laurie's character House, but without the acerbic wit that Laurie brings to House. Meanwhile, Tucci's 'straight guy', the emphatic doctor Seger, is not developed into a more interesting character, like the fallible 'straight guys' Cuddy and Wilson. Indira Varma's character Adrienne Holland is too similar to Jennifer Morrison's doctor Cameron to be a co-incidence.<br /><br />Someone at CBS obviously noticed the success of House, M.D. and told his staff to get him (her) a similar show, hoping that mimicry would prove successful. However, copying a show like House demands the same high level balance of wit and suspense and Tucci and company are just not up to the challenge.<br /><br />I didn't know the show was canceled until I read the comments on IMDb, but it doesn't come as a surprise to me.\">, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.evaluate(imdb_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaJAZdGlRHxa",
        "outputId": "103b5331-bd12-4ae4-c30b-8dfabddc4686"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1404s\u001b[0m 897ms/step - loss: 0.4635 - sparse_categorical_accuracy: 0.7852\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4630182087421417, 0.7834799885749817]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning a pretrained backbone"
      ],
      "metadata": {
        "id": "bMf1pPVULlB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=2,\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train,\n",
        "    validation_data=imdb_test,\n",
        "    epochs=1,\n",
        ")\n"
      ],
      "metadata": {
        "id": "FOdQv5oKvqxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd68d0cd-1638-447f-8814-a081d4572612"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/config.json...\n",
            "100%|██████████| 507/507 [00:00<00:00, 415kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/model.weights.h5...\n",
            "100%|██████████| 16.8M/16.8M [00:00<00:00, 87.0MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/tokenizer.json...\n",
            "100%|██████████| 547/547 [00:00<00:00, 828kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/assets/tokenizer/vocabulary.txt...\n",
            "100%|██████████| 226k/226k [00:00<00:00, 6.67MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5972s\u001b[0m 4s/step - loss: 0.5215 - sparse_categorical_accuracy: 0.7261 - val_loss: 0.3036 - val_sparse_categorical_accuracy: 0.8734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bad79186320>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning with user-controlled preprocessing"
      ],
      "metadata": {
        "id": "oeaFev6_Ln0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "# Apply the preprocessor to every sample of train and test data using `map()`.\n",
        "# `tf.data.AUTOTUNE` and `prefetch()` are options to tune performance, see\n",
        "# https://www.tensorflow.org/guide/data_performance for details.\n",
        "\n",
        "# Note: only call `cache()` if you training data fits in CPU memory!\n",
        "imdb_train_cached = (\n",
        "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "imdb_test_cached = (\n",
        "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train_cached,\n",
        "    validation_data=imdb_test_cached,\n",
        "    epochs=3,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jTx9menLvskX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8938fe-27e8-4bce-da7f-ee774d0a85d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5957s\u001b[0m 4s/step - loss: 0.5280 - sparse_categorical_accuracy: 0.7227 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.8733\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5866s\u001b[0m 4s/step - loss: 0.2854 - sparse_categorical_accuracy: 0.8822 - val_loss: 0.3195 - val_sparse_categorical_accuracy: 0.8719\n",
            "Epoch 3/3\n",
            "\u001b[1m1482/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3:58\u001b[0m 3s/step - loss: 0.2096 - sparse_categorical_accuracy: 0.9190"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning a custom modes  "
      ],
      "metadata": {
        "id": "BxztOuHnLqYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
        "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
        "\n",
        "imdb_train_preprocessed = (\n",
        "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "imdb_test_preprocessed = (\n",
        "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "backbone.trainable = False\n",
        "inputs = backbone.input\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "for _ in range(2):\n",
        "    sequence = keras_nlp.layers.TransformerEncoder(\n",
        "        num_heads=2,\n",
        "        intermediate_dim=512,\n",
        "        dropout=0.1,\n",
        "    )(sequence)\n",
        "# Use [CLS] token output to classify\n",
        "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(5e-5),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    imdb_train_preprocessed,\n",
        "    validation_data=imdb_test_preprocessed,\n",
        "    epochs=3,\n",
        ")\n"
      ],
      "metadata": {
        "id": "5lqYVrvlvuHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}